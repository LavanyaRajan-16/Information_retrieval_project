---
title: "219000429_GY7708_CW2"
Author: 219000429
output: html_notebook
---


```{r}
library(tidyverse)
library(tidytext)
library(magrittr)
library(jsonlite)
library(httr)
library(rvest)
library(htm2txt)
library(stringi)
library(WikipediR)
library(dplyr)
library(tm)
library(tokenizers)
library(sf)
library(raster)
library(cluster)
library(textdata)
library(wordcloud)

```
Part-1
```{r}
wiki_geo <- read.csv(file.choose())
```

```{r}
wiki_geo <- wiki_geo %>% 
  filter(LAD21NM == 'Wealden') %>% 
  filter(gt_primary == 1)
```
```{r}

head(wiki_geo)
```


```{r}

page_sent <- data.frame(
  page_name = character(),
  a_page_summary = character()
)

for ( a_page_title in wiki_geo$page_title){
# Retrieve the summary
a_page_summary <-
  httr::GET(
    # Base API URL
    url = "https://en.wikipedia.org/w/api.php",
    # API query definition
    query = list(
      # Use JSON data format
      format = "json",
      action = "query",
      # Only retrieve the intro
      prop = "extracts",
      exintro = 1,
      explaintext = 1,
      redirects = 1,
      # Set the title
      titles = a_page_title
    )
  ) %>%
  # Get the content
  httr::content(
    as = "text",
    encoding = "UTF-8"
  ) %>%
  # Trasnform JSON content to R list
  jsonlite::fromJSON() %>%
  # Extract the summary from the list
  magrittr::extract2("query") %>%
  magrittr::extract2("pages") %>%
  magrittr::extract2(1) %>%
  magrittr::extract2("extract")

a_page_summary <- as.data.frame(a_page_summary)
a_page_summary$page_name <- a_page_title
print (a_page_summary)
page_sent <- page_sent %>% 
  add_row(a_page_summary)
}
```

```{r}
wiki_geo_coord <- wiki_geo %>% 
  select(gt_lon, gt_lat, page_title)
```

```{r}
to_join <- wiki_geo %>% 
  select(page_title, gt_lat, gt_lon)
```


```{r}
leftJoinDf <- left_join(page_sent,to_join, by = c("page_name" = "page_title"))
View(leftJoinDf)
```

```{r}
glimpse(wiki_geo)
```

FREQUENCY OF WORDS


```{r}
wiki_text <- leftJoinDf %>% 
  unnest_tokens(word, a_page_summary)
```
Part- 2
```{r}
wiki_text%>% 
  anti_join(stop_words) %>% 
  count(word, sort = TRUE)
```

```{r}
frequency_dataframe = wiki_text %>% count(word) %>% arrange(desc(n))

```

```{r}
short_dataframe = head(frequency_dataframe, 20)
```

```{r}
ggplot(short_dataframe, aes(x = word, y = n, fill = word)) + geom_col()
```



```{r}
unnest_function <- function(page_name) {
  
  # Gets the page content
  page_info <- WikipediR::page_content('en', 'wikipedia', page_name = page_name)
  
  # Gets just the text
  page_info <- page_info$parse$text
  
  # Removes useless html code
  page_text <- htm2txt::htm2txt(page_info)
  
    # Removes some leftover characters
  page_text <- stri_replace_all_regex(page_text, "\n", "")
  page_text <- stri_replace_all_regex(page_text, "â¢", "")
  page_text <- stri_replace_all_regex(page_text, "\n\n", "")
  page_text <- stri_replace_all_regex(page_text, '/', "")
  page_text <- stri_replace_all_regex(page_text, '\"', "")
  
  # Changes it into a dataframe
  page_text <- as.data.frame(page_text)
  
  # Makes the dataframe into individual words
  unnested <- page_text %>% 
    unnest_tokens(word, page_text)
  
  # Removes 'stopwords' like 'The' as they are useless
  unnested <- unnested %>% 
    anti_join(get_stopwords()) %>%
    count(word, sort = TRUE)
  
  # adds the page title as a column
   unnested <- unnested %>% 
     dplyr::mutate(page_name)
  
  return(unnested)
}

```

```{r}
page_word_count <- data.frame(
  page_name = c(),
  word = c(),
  n = c()
)

```

```{r}
for (i in 1:nrow(wiki_text)) {
  print(paste(
    'Page title: Wealden',
    wiki_geo$page_title[i]
  ))
  
  page <- wiki_geo$page_title[i]
  
  page_word_count <- page_word_count %>% 
    bind_rows(unnest_function(page))
}

```

Clustering 
```{r}
kmeans <- wiki_text%>% 
  dplyr::select(gt_lat,gt_lon)%>%
  stats::kmeans(centers = 6,iter.max = 50)

cluster <-
  wiki_text %>%
  tibble::add_column(
    cluster = kmeans%$% cluster%>% as.character()
  )
```
```{r}
cluster %>% 
  filter(cluster == 3) %>% 
  anti_join(stop_words) %>% 
  count(word, sort = TRUE)
```

```{r}
cluster_geo <- cluster %>% 
  sf::st_as_sf(coords = c('gt_lon', 'gt_lat'))
```

```{r}
plot(cluster_geo['cluster'])
```

```{r}
freq_cluster <- tibble(
  cluster = character(),
  word = character(),
  n = integer()
)

for (cluster_no in unique(cluster$cluster)){
  freq_cluster <- freq_cluster %>% rbind(
    cluster %>%
      filter(cluster == cluster_no) %>%
      anti_join(stop_words) %>%
      count(word, cluster, sort = TRUE) %>%
      slice_head(n = 10)
  )
}
```

```{r}
freq_cluster %>% filter(cluster == '1') %>% 
  ggplot() +
    geom_bar(aes(x = word, y = n), stat = 'identity')+
    labs(title = 'Cluster 1')
```
```{r}
freq_cluster %>% filter(cluster == '2') %>% 
  ggplot() +
    geom_bar(aes(x = word, y = n), stat = 'identity')+
    labs(title = 'Cluster 2')
```

```{r}
freq_cluster %>% filter(cluster == '3') %>% 
  ggplot() +
    geom_bar(aes(x = word, y = n), stat = 'identity')+
    labs(title = 'Cluster 3')
```
```{r}
freq_cluster %>% filter(cluster == '4') %>% 
  ggplot() +
    geom_bar(aes(x = word, y = n), stat = 'identity')+
    labs(title = 'Cluster 4')
```
```{r}
freq_cluster %>% filter(cluster == '5') %>% 
  ggplot() +
    geom_bar(aes(x = word, y = n), stat = 'identity')+
    labs(title = 'Cluster 5')
```
```{r}
freq_cluster %>% filter(cluster == '6') %>% 
  ggplot() +
    geom_bar(aes(x = word, y = n), stat = 'identity') +
    labs(title = 'Cluster 6')
```

Part 3 
```{r}
wiki_text  %>%
  count(page_name)

```

```{r}
get_sentiments("bing")
```
```{r}
get_sentiments("nrc")
```
```{r}
get_sentiments("afinn")
```
```{r}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

wiki_text%>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```
```{r}
library(tidyr)

jane_austen_sentiment <- wiki_text %>%
  inner_join(get_sentiments("bing")) %>%
  count(page_name, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
print(jane_austen_sentiment)
```

```{r}
library(ggplot2)

ggplot(jane_austen_sentiment, aes(index, sentiment, fill = page_name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~page_name, ncol = 2, scales = "free_x")
```

```{r}
get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)
```

```{r}
get_sentiments("bing") %>% 
  count(sentiment)

```

```{r}
bing_word_counts <- wiki_text %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
```

```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

```{r}
custom_stop_words <- bind_rows(tibble(word = c("miss"),  
                                      lexicon = c("custom")), 
                               stop_words)
```

```{r}


wiki_text %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

```{r}
library(reshape2)

wiki_text %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```


```{r}
wiki_sentiment <- wiki_text %>%
  # implement sentiment analysis with the "bing" lexicon
  inner_join(get_sentiments("bing")) 

wiki_sentiment %>%
  # find how many positive/negative words each play has
  count(page_name, sentiment)
```

```{r}
sentiment_counts <- wiki_text %>%
  # implement sentiment analysis using the "bing" lexicon
  inner_join(get_sentiments("bing")) %>%
  # count the number of words by title, genre, and sentiment
  count(page_name,  sentiment)

sentiment_counts %>%
  group_by(page_name) %>%
  # find the total number of words in each play
  mutate(total = sum(n),
         percent = n / total) %>%
  # filter the results for only negative sentiment
  filter(sentiment == "negative") %>%
  arrange(percent)
```
 
```{r}
word_counts <- wiki_text %>%
  inner_join(get_sentiments("bing")) %>%
  # count by word and sentiment
  count(word, sentiment)

top_words <- word_counts %>%
  # group by sentiment
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n))

ggplot(top_words, aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ sentiment, scales = "free")
```
 
```{r}
wiki_text %>%
  # implement sentiment analysis using "bing" lexicon
  inner_join(get_sentiments("bing")) %>%
  # count using four arguments
  count(page_name, sentiment)
```
 
```{r}
wiki_text %>%
  inner_join(get_sentiments("bing")) %>%
   count(page_name, sentiment) %>%
  # pivot sentiment and n wider
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  # use mutate to compute net sentiment
  mutate(sentiment = positive - negative) %>%
  # put index on x-axis, sentiment on y-axis, and map comedy/tragedy to fill
  ggplot(aes(page_name, sentiment)) +
  # make a bar chart with geom_col()
  geom_col() +
  # make small multiples for each title with facet_wrap()
  facet_wrap(~ page_name, scales = "free_x")
```
```{r}
bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```
 
 